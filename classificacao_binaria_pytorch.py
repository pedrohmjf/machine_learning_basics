# -*- coding: utf-8 -*-
"""classificacao_binaria_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rub0lOyFbKxWBEY7W609zu7VZ5YMmPd5
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

import numpy as np
from matplotlib.pyplot import subplot, plot, show, clf, vlines
import matplotlib.pyplot as plt

"""### Conjunto de dados"""

mean0, std0 = -0.4, 0.5
mean1, std1 = 0.9, 0.3
m = 200

x1_train = np.random.randn(m//2) * std1 + mean1
x0_train = np.random.randn(m//2) * std0 + mean0
X_train = np.hstack((x1_train, x0_train))

y_train = np.hstack((np.ones(m//2), np.zeros(m//2)))

plot(X_train[:m//2], y_train[:m//2], '.')
plot(X_train[m//2:], y_train[m//2:], '.')
show()

thetas = [-3, 2]

"""### Convertendo para tensores"""

X_train = torch.Tensor(X_train)
y_train = torch.Tensor(y_train)

"""### Definindo o modelo"""

class BinClass(nn.Module):
  # Construtor
  def __init__(self):
    super(BinClass, self).__init__()
    self.fc1 = nn.Linear(200, 1)
    self.sigmoid = nn.Sigmoid()

  # Propagação
  def forward(self, x):
      x = F.relu(self.fc1(x))
      x = self.sigmoid(x)
      return x

model = BinClass()
print(model)

"""### Treinando o modelo"""

model = BinClass()
loss_function = nn.BCELoss()
learning_rate = 1e-6
epochs = 100

# Otimizador SGD
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

for t in range(epochs):
  # Coloca em modo treinamento (calcular gradientes)
  model.train()

  # Propagação
  y_pred = model(X_train)

  # Calcular erro com a função de custo
  loss = loss_function(y_pred, y_train)
  print(t, loss.item())

  # Zera os gradientes antes da Retro-propagação (Backpropagation)
  model.zero_grad()

  # Retro-propagação (Backpropagation)
  loss.backward()

  # Atualização dos parâmetros
  optimizer.step()

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Define a classe da rede neural
class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        out = self.sigmoid(out)
        return out

# Define os parâmetros da rede neural
input_size = 2
hidden_size = 5
learning_rate = 0.1
num_epochs = 1000

# Cria o conjunto de dados de treinamento
X = torch.Tensor([[0, 0], [0, 1], [1, 0], [1, 1]])
Y = torch.Tensor([[0], [1], [1], [0]])

# Instancia a rede neural
net = NeuralNet(input_size, hidden_size)

# Define a função de perda e o otimizador
criterion = nn.BCELoss()
optimizer = optim.SGD(net.parameters(), lr=learning_rate)

# Treina a rede neural
for epoch in range(num_epochs):
    # Forward pass
    outputs = net(X)
    loss = criterion(outputs, Y)

    # Backward pass e otimização
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Printa o progresso do treinamento a cada 100 epochs
    if (epoch+1) % 100 == 0:
        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Testa a rede neural com novos dados
test_input = torch.Tensor([[0, 0], [0, 1], [1, 0], [1, 1]])
test_output = net(test_input)
predictions = (test_output > 0.5).float()
print("Predictions:\n", predictions)

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# gerando conjunto de dados de treinamento aleatório
train_data = torch.rand(1000)
train_labels = torch.where(train_data > 0.6, torch.tensor(1), torch.tensor(0))

# definindo a arquitetura da rede neural
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(1, 16)
        self.fc2 = nn.Linear(16, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        return x

net = Net()

# definindo a função de perda e o otimizador
criterion = nn.BCELoss()
optimizer = optim.Adam(net.parameters(), lr=0.01)

# loop de treinamento
for epoch in range(100):
    # passando o conjunto de dados de treinamento pela rede neural
    outputs = net(train_data.unsqueeze(1).float())

    # calculando a perda
    loss = criterion(outputs, train_labels.unsqueeze(1).float())

    # ajustando os pesos
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # calculando a acurácia
    predicted = torch.round(outputs)
    accuracy = (predicted == train_labels.unsqueeze(1).float()).sum()
    
    # plotando o gráfico com os dados e a fronteira de decisão
    if epoch % 10 == 0 or epoch == 99:
        plt.clf()
        plt.scatter(train_data[train_labels==0], train_labels[train_labels==0], color='blue', marker='o', label='benigno')
        plt.scatter(train_data[train_labels==1], train_labels[train_labels==1], color='red', marker='x', label='maligno')
        plt.plot(torch.linspace(0, 1, 100), net(torch.linspace(0, 1, 100).unsqueeze(1)).detach().numpy(), color='black')
        plt.title(f'Época: {epoch+1}, Acurácia: {accuracy.item()/len(train_labels):.2f}')
        plt.xlabel('Tamanho do tumor')
        plt.ylabel('Rótulo')
        plt.xlim([0, 1])
        plt.ylim([-0.1, 1.1])
        plt.legend()
        plt.pause(0.1)

# exibindo o gráfico final
plt.clf()
plt.scatter(train_data[train_labels==0], train_labels[train_labels==0], color='blue', marker='o', label='benigno')
plt.scatter(train_data[train_labels==1], train_labels[train_labels==1], color='red', marker='x', label='maligno')
plt.plot(torch.linspace(0, 1, 100), net(torch.linspace(0, 1, 100).unsqueeze(1)).detach().numpy(), color='black')
plt.title(f'Época: {epoch+1}, Acurácia: {accuracy.item()/len(train_labels):.2f}')
plt.xlabel('Tamanho do tumor')
plt.ylabel('Rótulo')
plt.xlim([0, 1])
plt.ylim([-0.1, 1.1])
plt.legend()
plt.show()