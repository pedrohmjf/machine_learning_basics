# -*- coding: utf-8 -*-
"""regressão.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r5EkE1TxUb9mkDojZk9SazG07oGdQb1f
"""

import numpy as np
import matplotlib.pyplot as plt

def f_true(x):
  return 2 + 0.8 * x

# Conjunto de dados {(x,y)}
xs = np.linspace(-3, 3, 100)
ys = np.array([f_true(x) + np.random.randn()*0.5 for x in xs])

thetas = [4, 2]

# Hipotese
def h(x, theta):
  return theta[0] + theta[1] * x


# Funcao de custo
def J(theta, xs, ys):
  m = len(xs)
  soma = 0
  for i in range(m):
    custo = (h(xs[i], theta) - ys[i]) ** 2
    soma = soma + custo
  custo_total = (1 / (2 * m)) * soma
  return custo_total
    

# Derivada parcial com respeito a theta[i]
def gradient(i, theta, xs, ys):
  m = len(xs)
  grad = 0
  if i == 0:  
    for j in range(m):  
      grad = grad + (h(xs[j], theta) - ys[j])
  if i == 1:
    for j in range(m):  
      grad = grad + (h(xs[j], theta) - ys[j]) * xs[j]      
  return (1 / m) * grad

# Gradient Descent
def gradient_descent(theta, xs, ys, alpha):
  m = len(ys)
  J_passado = float('inf')
  epsilon = float('inf')
  while epsilon > 10**(-9):
    temp0 = theta[0] - alpha * gradient(0, theta, xs, ys)
    temp1 = theta[1] - alpha * gradient(1, theta, xs, ys)
    theta[0] = temp0
    theta[1] = temp1
    J_atual = J(theta, xs, ys)
    epsilon = J_passado - J_atual
    J_passado = J_atual
  return theta

'''
Plota, no mesmo grafico:

- O modelo / hipotese (reta)
- A reta original (true function)
- Os dados com ruido (xs, ys)
'''
def print_modelo(theta, xs, ys):
  h_xs = [h(x, theta) for x in xs]
  f_xs = [f_true(x) for x in xs]

  plt.plot(xs, ys, 'o', label='Dados com ruído')
  plt.plot(xs, h_xs, label='Hipótese')
  plt.plot(xs, f_xs, label='Reta original')
  plt.legend()
  plt.show()

print(gradient_descent(thetas, xs, ys, 0.0001))
print_modelo(thetas, xs, ys)