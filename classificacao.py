# -*- coding: utf-8 -*-
"""classificacao.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lTO0TR-cfB17ilyfwxuJATktQo-o6bD8
"""

import numpy as np
from matplotlib.pyplot import subplot, plot, show, clf, vlines
import matplotlib.pyplot as plt

# Conjunto de dados {(x,y)}
mean0, std0 = -0.4, 0.5
mean1, std1 = 0.9, 0.3
m = 200

x1s = np.random.randn(m//2) * std1 + mean1
x0s = np.random.randn(m//2) * std0 + mean0
xs = np.hstack((x1s, x0s))

ys = np.hstack((np.ones(m//2), np.zeros(m//2)))

plot(xs[:m//2], ys[:m//2], '.')
plot(xs[m//2:], ys[m//2:], '.')
show()

thetas = [-3, 2]

# Função sigmoid
def sigmoid(z):
  return 1 / (1 + np.exp(-z))


# Hipotese -> sigmoid(theta[0] + theta[1] * x)
def h(x, theta):
  return sigmoid(theta[0] + theta[1] * x)

# Funcao de custo para um exemplo; entropia cruzada
def cost(h, y):
  if y == 0:
    return - np.log(1 - h)
  if y == 1:
    return - np.log(h)

# Funcao de custo total
def J(theta, xs, ys):
  m = len(xs)
  soma = 0
  for i in range(m):
    soma += cost(h(xs[i], theta), ys[i])
  return (1 / m) * soma

# Derivada parcial com respeito a theta [i]
def gradient(i, theta, xs, ys):
  m = len(xs)
  soma = 0
  for j in range(m):
    soma += (h(xs[j], theta) - ys[j]) * xs[j] if i == 1 else (h(xs[j], theta) - ys[j])
  return (1 / m) * soma

# Gradient Descent
def gradient_descent(theta, xs, ys, alpha):
  m = len(ys)
  J_passado = float('inf')
  epsilon = float('inf')
  while epsilon > 10**(-9):
    temp0 = theta[0] - alpha * gradient(0, theta, xs, ys)
    temp1 = theta[1] - alpha * gradient(1, theta, xs, ys)
    theta[0] = temp0
    theta[1] = temp1
    J_atual = J(theta, xs, ys)
    epsilon = J_passado - J_atual
    J_passado = J_atual
  return theta

# Plot fronteira de decisão
def plot_fronteira(theta):
  # Encontra o valor de x que faz a hipótese ser igual a 0.5
    x_decision = - theta[0] / theta[1]
    
  # Plota a fronteira de decisão como uma linha vertical
    plt.vlines(x_decision, ymin=0, ymax=1, color='r')

'''
Plota, em subplots: 
- Os dados, com a fronteira de decisao
- Os dados classificados
'''
def print_modelo(theta, xs, ys):
  subplot(1,2,1)
  plot(xs[ys==0], ys[ys==0], '.')
  plot(xs[ys==1], ys[ys==1], '.')
  plt.title('Dados classificados')
  
  subplot(1,2,2)
  plot(xs[ys==0], ys[ys==0], '.')
  plot(xs[ys==1], ys[ys==1], '.')
  plot_fronteira(theta)  # plota a fronteira de decisão
  plt.title('Fronteira de decisão')
  
  show()

def accuracy (ys, predictions):
  num = sum (ys == predictions)
  return num / len(ys)


alpha = 0.1 # completar
#epochs = 600
theta = [1, 1] # completar

print(gradient_descent(theta, xs, ys, alpha))
print_modelo(theta, xs, ys)

'''for k in range(epochs) : # 10000

  # apply gradient decent

  # show classication performance
  print('Acuracia : ', accuracy(ys, predictions))
'''